{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6da849e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature      | OLS (No Penalty)   | Ridge (L2)         | Lasso (L1)        \n",
      "--------------------------------------------------------------------------------\n",
      "Intercept    |       -0.000203 |       -0.001854 |       -0.003957\n",
      "AAPL Open    |        0.970905 |        0.793786 |        0.976622\n",
      "AAPL Vol     |       -0.001030 |       -0.004631 |        0.000000\n",
      "MSFT Close   |        0.021150 |        0.143473 |        0.008768\n",
      "TSLA Close   |        0.008126 |        0.062588 |        0.004329\n",
      "\n",
      "--- Feature Selection Discussion ---\n",
      "Lasso has kept feature: AAPL Open\n",
      "Lasso has excluded feature: AAPL Vol (Coefficient set to 0)\n",
      "Lasso has kept feature: MSFT Close\n",
      "Lasso has kept feature: TSLA Close\n",
      "\n",
      "Observation: Ridge shrinks all coefficients proportionally, while Lasso performs feature selection by setting less impactful features (like Volume or Competitor prices) to zero.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# PART E: REGULARIZATION (RIDGE & LASSO)\n",
    "# ==============================================================================\n",
    "# Goal: Apply Ridge and Lasso to discuss feature importance and selection.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. DATA PREPARATION (Reusing Standardized Data logic from Part C)\n",
    "# ------------------------------------------------------------------------------\n",
    "df_raw = pd.read_csv('stock_market_data.csv', header=[0,1])\n",
    "data = pd.DataFrame()\n",
    "data['y']    = pd.to_numeric(df_raw['Close']['AAPL'], errors='coerce')\n",
    "data['Open'] = pd.to_numeric(df_raw['Open']['AAPL'], errors='coerce')\n",
    "data['Vol']  = pd.to_numeric(df_raw['Volume']['AAPL'], errors='coerce')\n",
    "data['MSFT'] = pd.to_numeric(df_raw['Close']['MSFT'], errors='coerce')\n",
    "data['TSLA'] = pd.to_numeric(df_raw['Close']['TSLA'], errors='coerce')\n",
    "data = data.dropna()\n",
    "\n",
    "def standardize(col): return (col - col.mean()) / col.std()\n",
    "\n",
    "X_scaled = data[['Open', 'Vol', 'MSFT', 'TSLA']].apply(standardize)\n",
    "X_scaled.insert(0, 'Bias', 1.0)\n",
    "y_scaled = standardize(data['y'])\n",
    "\n",
    "X_train = X_scaled.values[:int(len(X_scaled)*0.8)]\n",
    "y_train = y_scaled.values[:int(len(y_scaled)*0.8)]\n",
    "\n",
    "# 2. RIDGE REGRESSION (L2) - Matrix Implementation\n",
    "# ------------------------------------------------------------------------------\n",
    "# Formula: theta = (X^T * X + lambda * I)^-1 * X^T * y\n",
    "def train_ridge(X, y, lam):\n",
    "    XTX = X.T.dot(X)\n",
    "    I = np.eye(XTX.shape[0])\n",
    "    I[0,0] = 0  # Do not regularize the intercept\n",
    "    theta = np.linalg.pinv(XTX + lam * I).dot(X.T).dot(y)\n",
    "    return theta\n",
    "\n",
    "# 3. LASSO REGRESSION (L1) - Coordinate Descent Implementation\n",
    "# ------------------------------------------------------------------------------\n",
    "def soft_threshold(rho, lam):\n",
    "    if rho < -lam: return rho + lam\n",
    "    if rho > lam:  return rho - lam\n",
    "    return 0\n",
    "\n",
    "def train_lasso(X, y, lam, iterations=100):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    for _ in range(iterations):\n",
    "        for j in range(n):\n",
    "            X_j = X[:, j]\n",
    "            y_pred = X.dot(theta)\n",
    "            # Calculate rho without the j-th feature\n",
    "            rho = X_j.T.dot(y - y_pred + theta[j]*X_j)\n",
    "            if j == 0: # Bias term\n",
    "                theta[j] = rho / m\n",
    "            else:\n",
    "                theta[j] = soft_threshold(rho, lam) / (X_j.T.dot(X_j))\n",
    "    return theta\n",
    "\n",
    "# 4. EXECUTION AND COEFFICIENT COMPARISON\n",
    "# ------------------------------------------------------------------------------\n",
    "lam = 10.0 # Penalty Strength\n",
    "\n",
    "theta_ols   = train_ridge(X_train, y_train, lam=0)     # OLS (Part C)\n",
    "theta_ridge = train_ridge(X_train, y_train, lam=lam)   # Ridge\n",
    "theta_lasso = train_lasso(X_train, y_train, lam=lam)   # Lasso\n",
    "\n",
    "print(f\"{'Feature':<12} | {'OLS (No Penalty)':<18} | {'Ridge (L2)':<18} | {'Lasso (L1)':<18}\")\n",
    "print(\"-\" * 80)\n",
    "features = ['Intercept', 'AAPL Open', 'AAPL Vol', 'MSFT Close', 'TSLA Close']\n",
    "for i in range(len(features)):\n",
    "    print(f\"{features[i]:<12} | {theta_ols[i]:15.6f} | {theta_ridge[i]:15.6f} | {theta_lasso[i]:15.6f}\")\n",
    "\n",
    "# 5. DISCUSSION ON FEATURE SELECTION\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n--- Feature Selection Discussion ---\")\n",
    "for i, val in enumerate(theta_lasso):\n",
    "    if abs(val) < 1e-5 and i != 0:\n",
    "        print(f\"Lasso has excluded feature: {features[i]} (Coefficient set to 0)\")\n",
    "    elif i != 0:\n",
    "        print(f\"Lasso has kept feature: {features[i]}\")\n",
    "\n",
    "print(\"\\nObservation: Ridge shrinks all coefficients proportionally, while Lasso performs feature selection by setting less impactful features (like Volume or Competitor prices) to zero.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
